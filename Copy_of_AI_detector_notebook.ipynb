{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+KEmSal6u2T3pe1GCM732",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buriihenry/AI-detector-and-Plag-Checker/blob/master/Copy_of_AI_detector_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8eoVghmFLWj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sentence-transformers nltk flask shap scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xnxXylGK6oZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OZ8g1qDNQ3r",
        "outputId": "b96de7ee-0171-4b6c-f449-a0ec596af93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from typing import Dict\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "nltk.download('punkt')\n",
        "\n",
        "class AIDetector:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.class_weights = None\n",
        "        self.optimal_threshold = 0.5\n",
        "        self.text_generator = None\n",
        "\n",
        "        # Initialize models with dropout\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "            \"roberta-base\",\n",
        "            num_labels=2,\n",
        "            hidden_dropout_prob=0.7,\n",
        "            attention_probs_dropout_prob=0.5\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "        # Configuration\n",
        "        self.max_length = 512\n",
        "        self.batch_size = 4  # Reduced for better gradient updates\n",
        "        self.num_epochs = 5\n",
        "        self.weight_decay = 0.1  # Strong L2 regularization\n",
        "\n",
        "    def _augment_ai_texts(self, original_texts: list, num_samples: int = 1000) -> list:\n",
        "        \"\"\"Generate synthetic AI texts using GPT-2\"\"\"\n",
        "        try:\n",
        "            if not self.text_generator:\n",
        "                self.text_generator = pipeline(\n",
        "                    'text-generation',\n",
        "                    model='gpt2-medium',\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "\n",
        "            augmented = []\n",
        "            for text in tqdm(original_texts, desc=\"Augmenting AI texts\"):\n",
        "                generated = self.text_generator(\n",
        "                    text,\n",
        "                    max_length=self.max_length,\n",
        "                    num_return_sequences=5,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.9,\n",
        "                    top_k=50,\n",
        "                    repetition_penalty=1.2\n",
        "                )\n",
        "                augmented.extend([g['generated_text'] for g in generated])\n",
        "\n",
        "                if len(augmented) >= num_samples:\n",
        "                    break\n",
        "\n",
        "            return augmented[:num_samples]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Text generation failed: {str(e)}\")\n",
        "            return original_texts * 200  # Fallback replication\n",
        "\n",
        "    def load_data(self, file_path: str):\n",
        "        \"\"\"Load and balance dataset with synthetic AI texts\"\"\"\n",
        "        try:\n",
        "            # Load original data\n",
        "            df = pd.read_csv(file_path)\n",
        "            human_texts = df[df['generated'] == 0]['text'].tolist()\n",
        "            ai_texts = df[df['generated'] == 1]['text'].tolist()\n",
        "\n",
        "            # Generate synthetic AI texts\n",
        "            logging.info(f\"Original AI texts: {len(ai_texts)}\")\n",
        "            augmented_ai = self._augment_ai_texts(ai_texts, num_samples=len(human_texts))\n",
        "            logging.info(f\"Generated {len(augmented_ai)} synthetic AI texts\")\n",
        "\n",
        "            # Create balanced dataset\n",
        "            balanced_texts = human_texts + augmented_ai\n",
        "            balanced_labels = [0]*len(human_texts) + [1]*len(augmented_ai)\n",
        "\n",
        "            # Stratified split\n",
        "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "                balanced_texts, balanced_labels,\n",
        "                test_size=0.3, stratify=balanced_labels, random_state=42\n",
        "            )\n",
        "            X_val, X_test, y_val, y_test = train_test_split(\n",
        "                X_temp, y_temp,\n",
        "                test_size=0.5, stratify=y_temp, random_state=42\n",
        "            )\n",
        "\n",
        "            # Assign to instance variables\n",
        "            self.X_train, self.y_train = X_train, y_train\n",
        "            self.X_val, self.y_val = X_val, y_val\n",
        "            self.X_test, self.y_test = X_test, y_test\n",
        "\n",
        "            # Calculate aggressive class weights\n",
        "            pos_weight = len(y_train)/sum(y_train) if sum(y_train) > 0 else 1000\n",
        "            self.class_weights = torch.tensor([1.0, pos_weight], device=self.device)\n",
        "\n",
        "            logging.info(\"\\nBalanced Dataset Statistics:\")\n",
        "            logging.info(f\"Training: {len(y_train)} (AI: {sum(y_train)})\")\n",
        "            logging.info(f\"Validation: {len(y_val)} (AI: {sum(y_val)})\")\n",
        "            logging.info(f\"Testing: {len(y_test)} (AI: {sum(y_test)})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Data loading failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _create_dataloader(self, texts: list, labels: list = None):\n",
        "        \"\"\"Create tensor dataloaders\"\"\"\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        if labels is not None:\n",
        "            dataset = TensorDataset(\n",
        "                encodings['input_ids'],\n",
        "                encodings['attention_mask'],\n",
        "                torch.tensor(labels)\n",
        "            )\n",
        "            shuffle = True\n",
        "        else:\n",
        "            dataset = TensorDataset(\n",
        "                encodings['input_ids'],\n",
        "                encodings['attention_mask']\n",
        "            )\n",
        "            shuffle = False\n",
        "\n",
        "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Training process with aggressive class weighting\"\"\"\n",
        "        try:\n",
        "            train_loader = self._create_dataloader(self.X_train, self.y_train)\n",
        "            val_loader = self._create_dataloader(self.X_val, self.y_val)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                self.model.parameters(),\n",
        "                lr=1e-5,\n",
        "                weight_decay=self.weight_decay\n",
        "            )\n",
        "            criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "            best_f1 = 0\n",
        "            patience = 0\n",
        "\n",
        "            for epoch in range(self.num_epochs):\n",
        "                # Training\n",
        "                self.model.train()\n",
        "                total_loss = 0\n",
        "                for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                    inputs = {\n",
        "                        'input_ids': batch[0].to(self.device),\n",
        "                        'attention_mask': batch[1].to(self.device),\n",
        "                        'labels': batch[2].to(self.device)\n",
        "                    }\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.model(**inputs)\n",
        "                    loss = criterion(outputs.logits, inputs['labels'])\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # Validation\n",
        "                val_probs, val_labels = [], []\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        inputs = {\n",
        "                            'input_ids': batch[0].to(self.device),\n",
        "                            'attention_mask': batch[1].to(self.device)\n",
        "                        }\n",
        "                        outputs = self.model(**inputs)\n",
        "                        val_probs.extend(F.softmax(outputs.logits, dim=1)[:,1].cpu().numpy())\n",
        "                        val_labels.extend(batch[2].cpu().numpy())\n",
        "\n",
        "                # Threshold tuning\n",
        "                precision, recall, thresholds = precision_recall_curve(val_labels, val_probs)\n",
        "                f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "                best_idx = np.argmax(f1_scores)\n",
        "                self.optimal_threshold = thresholds[best_idx]\n",
        "\n",
        "                current_f1 = f1_scores[best_idx]\n",
        "                logging.info(f\"\\nEpoch {epoch+1}:\")\n",
        "                logging.info(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
        "                logging.info(f\"Val F1: {current_f1:.4f}\")\n",
        "                logging.info(f\"Optimal Threshold: {self.optimal_threshold:.4f}\")\n",
        "\n",
        "                # Early stopping\n",
        "                if current_f1 > best_f1:\n",
        "                    best_f1 = current_f1\n",
        "                    patience = 0\n",
        "                    torch.save(self.model.state_dict(), \"best_model.pt\")\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= 2:\n",
        "                        logging.info(\"Early stopping triggered\")\n",
        "                        break\n",
        "\n",
        "            # Load best model\n",
        "            self.model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Training failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Comprehensive evaluation with AI detection metrics\"\"\"\n",
        "        test_loader = self._create_dataloader(self.X_test)\n",
        "        probs, preds = [], []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                inputs = {\n",
        "                    'input_ids': batch[0].to(self.device),\n",
        "                    'attention_mask': batch[1].to(self.device)\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                batch_probs = F.softmax(outputs.logits, dim=1)\n",
        "                probs.extend(batch_probs[:,1].cpu().numpy())\n",
        "                preds.extend((batch_probs[:,1] >= self.optimal_threshold).cpu().numpy().astype(int))\n",
        "\n",
        "        # AI Detection Metrics\n",
        "        ai_indices = np.where(np.array(self.y_test) == 1)[0]\n",
        "        human_indices = np.where(np.array(self.y_test) == 0)[0]\n",
        "\n",
        "        ai_detection_rate = sum(np.array(preds)[ai_indices])/len(ai_indices)\n",
        "        human_fp_rate = sum(np.array(preds)[human_indices])/len(human_indices)\n",
        "\n",
        "        logging.info(\"\\nFinal Evaluation:\")\n",
        "        logging.info(f\"AI Detection Rate: {ai_detection_rate:.2%}\")\n",
        "        logging.info(f\"Human False Positive Rate: {human_fp_rate:.2%}\")\n",
        "        logging.info(\"\\nClassification Report:\")\n",
        "        logging.info(classification_report(self.y_test, preds))\n",
        "        logging.info(\"\\nConfusion Matrix:\")\n",
        "        logging.info(confusion_matrix(self.y_test, preds))\n",
        "\n",
        "    def detect(self, text: str) -> Dict:\n",
        "        \"\"\"Enhanced detection with similarity checks\"\"\"\n",
        "        try:\n",
        "            # Model prediction\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=self.max_length,\n",
        "                truncation=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = F.softmax(outputs.logits, dim=1)\n",
        "                ai_prob = probs[0][1].item()\n",
        "\n",
        "            # Similarity analysis\n",
        "            text_embedding = self.sentence_model.encode([text])[0]\n",
        "            ref_embeddings = self.sentence_model.encode(\n",
        "                self.X_train[:500] + self.X_train[-500:]  # Balanced references\n",
        "            )\n",
        "            similarities = F.cosine_similarity(\n",
        "                torch.tensor(text_embedding).unsqueeze(0),\n",
        "                torch.tensor(ref_embeddings)\n",
        "            )\n",
        "            similarity_score = similarities.mean().item()\n",
        "\n",
        "            return {\n",
        "                'is_ai_generated': ai_prob >= self.optimal_threshold,\n",
        "                'confidence': ai_prob,\n",
        "                'similarity_score': similarity_score,\n",
        "                'threshold': self.optimal_threshold\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Detection error: {str(e)}\")\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'is_ai_generated': None,\n",
        "                'confidence': None,\n",
        "                'similarity_score': None\n",
        "            }\n",
        "\n",
        "# Usage Example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdoZNGYbOHc0",
        "outputId": "241b8f48-53c9-457a-ebb3-2ad5de62702a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEORZ5YHFDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/Colab Notebooks/data_train.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC3lA22IOlKr",
        "outputId": "082619ce-897f-4338-ffb0-99cce17e0cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/Colab Notebooks/data_train.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage Example\n",
        "if __name__ == \"__main__\":\n",
        "    detector = AIDetector()\n",
        "\n",
        "    try:\n",
        "        detector.load_data(\"/content/drive/My Drive/Colab Notebooks/data_train.csv\")\n",
        "        detector.train()\n",
        "        detector.evaluate()\n",
        "\n",
        "        test_text = \"The rapid development of artificial intelligence has revolutionized numerous industries...\"\n",
        "        results = detector.detect(test_text)\n",
        "\n",
        "        print(\"\\nDetection Results:\")\n",
        "        print(f\"Prediction: {'AI-Generated' if results['is_ai_generated'] else 'Human'}\")\n",
        "        print(f\"Confidence: {results['confidence']:.2%}\")\n",
        "        print(f\"Similarity Score: {results['similarity_score']:.2f}\")\n",
        "        print(f\"Threshold: {results['threshold']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline error: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J0HG_qDSx1A",
        "outputId": "83a75e75-e9e4-4313-938c-ef379fe82ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Augmenting AI texts:   0%|          | 0/3 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Augmenting AI texts:  33%|███▎      | 1/3 [00:05<00:10,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Augmenting AI texts:  67%|██████▋   | 2/3 [00:09<00:04,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Augmenting AI texts: 100%|██████████| 3/3 [00:13<00:00,  4.61s/it]\n",
            "Epoch 1: 100%|██████████| 244/244 [01:38<00:00,  2.48it/s]\n",
            "Epoch 2: 100%|██████████| 244/244 [01:39<00:00,  2.44it/s]\n",
            "Epoch 3: 100%|██████████| 244/244 [01:40<00:00,  2.42it/s]\n",
            "<ipython-input-2-a63891e57ae8>:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(\"best_model.pt\"))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detection Results:\n",
            "Prediction: AI-Generated\n",
            "Confidence: 0.45%\n",
            "Similarity Score: 0.13\n",
            "Threshold: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "TX19pYlIUFET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"Analyze text and return formatted results\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to analyze.\"\n",
        "\n",
        "    results = detector.detect(text)\n",
        "\n",
        "    # Format results into a nice output\n",
        "    output = \"📊 Analysis Results:\\n\\n\"\n",
        "\n",
        "    if 'error' in results:\n",
        "        return f\"Error: {results['error']}\"\n",
        "\n",
        "    output += f\"🤖 AI Generated: {'Yes ✓' if results['is_ai_generated'] else 'No ✗'}\\n\"\n",
        "    output += f\"🎯 Confidence: {results['confidence']*100:.1f}%\\n\"\n",
        "    output += f\"🔄 Similarity Score: {results['similarity_score']:.2f}\\n\\n\"\n",
        "\n",
        "    # Add interpretation\n",
        "    output += \"💡 Interpretation:\\n\"\n",
        "    if results['is_ai_generated']:\n",
        "        if results['confidence'] > 0.9:\n",
        "            output += \"This text shows strong indicators of being AI-generated.\"\n",
        "        else:\n",
        "            output += \"This text shows some indicators of being AI-generated, but with moderate confidence.\"\n",
        "    else:\n",
        "        if results['confidence'] < 0.3:\n",
        "            output += \"This text shows strong indicators of being human-written.\"\n",
        "        else:\n",
        "            output += \"This text shows some indicators of being human-written, but with moderate confidence.\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Create the interface\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            lines=8,\n",
        "            label=\"Enter text to analyze\",\n",
        "            placeholder=\"Paste or type the text you want to analyze here...\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(lines=10, label=\"Analysis Results\"),\n",
        "    title=\"AI Text Detector\",\n",
        "    description=\"This tool analyzes text to determine if it was written by AI or a human.\",\n",
        "    examples=[\n",
        "        [\"The quick brown fox jumps over the lazy dog. This is a sample human-written text that you can use to test the detector.\"],\n",
        "        [\"In examining the fundamental principles of quantum mechanics, we observe that particles exhibit both wave-like and particle-like properties, a phenomenon known as wave-particle duality.\"]\n",
        "    ],\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "St334ZhjXiAF",
        "outputId": "4afc06aa-0356-4d91-e964-3bfe4f07c3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b6544eb47a50102d12.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b6544eb47a50102d12.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eB_EzPUBX7gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}